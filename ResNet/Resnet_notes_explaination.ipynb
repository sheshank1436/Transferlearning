{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a ResNet Neural Network?\n",
    "\n",
    "Residual Network (ResNet) is a Convolutional Neural Network (CNN)  architecture which was designed to enable hundreds or thousands of convolutional layers without the degradation in the perfomance of the model. While previous CNN architectures had a drop off in the effectiveness of additional layers, ResNet can add a large number of layers with strong performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the celebrated victory of AlexNet(1)  at the LSVRC2012 classification contest, deep Residual Network(2) was arguably the most groundbreaking work in the computer vision/deep learning community in the last few years.  \n",
    "\n",
    "When we studied about the Alexnet and VGG we saw how the layers have been stacked with one another to give the output.\n",
    "Alexnet had 8 layers that learn parametets, VGG had 16 layers that learned parameters.\n",
    "when we look into all these the fundamental structure doesn't change i.e convolution layers  followed by maxpooling and flatten layers at the end.AlexNet had only 5 convolutional layers, the VGG network(3) and GoogleNet (also codenamed Inception_v1)(4) had 19 and 22 layers respectively.\n",
    "\n",
    "\n",
    "\n",
    "But Resnet is far different from the other Architectures and it changed the way deeplearning works and this Architecture is being used in many use cases and it solved a lot of complicated problems and has evolved as one of the best arctictures of deep learning.\n",
    "\n",
    "Initially we had the intiution that as the number of layers increase in the network, the accuracy also increases in the same way, which doesn't seem to be right when we implement the deep learning models with large no. of layers  because as the layers increase the parameters also increase in the network and so the network calculations become complex and eventually the model not only becomes slow but also reduces the perfomance of the model.\n",
    "\n",
    "\n",
    "However, increasing network depth does not work by simply stacking layers together. Deep networks are hard to train because of the notorious vanishing gradient problem — as the gradient is back-propagated to earlier layers, repeated multiplication may make the gradient infinitively small. As a result, as the network goes deeper, its performance gets saturated or even starts degrading rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the below figure the error rate of a network with 54 layers is high  compared to the network with the network having 20 layers. We atleast expect that the network with more layers has less error in training phase but that doesn't seem to be true either in test or training phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![beforeresnet](resnet_ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet was an innovative solution to the “vanishing gradient” problem. Neural networks train via the backpropagation process, which relies on gradient descent, moving down the loss function to find the weights that minimize it. If there are too many layers, repeated multiplication makes the gradient smaller and smaller, until it “disappears”(becomes zero), causing performance to saturate or even degrade with each additional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the below diagram consider the VGG19 and 34-layer plain we notice that the 34-layer plain consists of several convolution layers\n",
    "stacked with one another,containng one max-pool layer in the initial phase and one-avg pool layer at the end. where as the VGG network has less layers compared to the other.\n",
    "But when we take a close look into both the networks there are some similarties between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we can see that the ResNet (the one on the right) consists on one convolution and pooling step (on orange) followed by 4 layers of similar behavior.\n",
    "Each of the layers follow the same pattern. They perform 3x3 convolution with a fixed feature map dimension (F) [64, 128, 256, 512] respectively, bypassing the input every 2 convolutions. Furthermore, the width (W) and height (H) dimensions remain constant during the entire layer.\n",
    "The dotted line is there, precisely because there has been a change in the dimension of the input volume (of course a reduction because of the convolution). Note that this reduction between layers is achieved by an increase on the stride, from 1 to 2, at the first convolution of each layer; instead of by a pooling operation, which we are used to see as down samplers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With any network, we compute the error gradient at the end of the network and use backpropagation to propagate our error gradient backwards through the network. Using the chain rule, we have to keep multiplying terms with the error gradient as we go backwards. However, in the long chain of multiplication, if we multiply many things together that are less than 1, then the final result will be very small. This applies to the gradient as well: the gradient becomes very small as we approach the earlier layers in a deep architecture. This small gradient is an issue because then we can’t update the network parameters by a large enough amount and training is very slow. In some cases, the gradient actually becomes zero, meaning we don’t update the earlier parameters at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we backpropagate through an operation, we have to use the chain rule and multiply, but what if we were to backpropagate through the identity function? Then the gradient would simply be multiplied by 1 and nothing would happen to it! This is the idea behind ResNet: it stacks these residual blocks together where we use an identity function to preserve the gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![resnet architecture](resnet_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why resnets work well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from below figure let us consider:\n",
    "\n",
    "\n",
    "x=a[l] then ouput of the first layer is a[l+1] and output of the second layer is a[l+2].<br>\n",
    "we mostly  use relu as the activation function in all the hidden layers of CNN models.<br>\n",
    "a[l+2]=relu((a[l+1]*w[l+2]+b[l+2])+a[l]) --------------------( 1) where<br>\n",
    "a[l+1]=relu((a[l]*w[l+1]+b[l+1]))<br>\n",
    "a[l+1] is F(x) and x is a[l]<br>\n",
    "                                          H(x)=F(x)+x     <br>\n",
    "we can replace a[l] with a[l]Ws\n",
    "but in skip connection the Ws is an identity function which means it doesn't have any parameters.\n",
    "if the dimensions of input and the output are same then we Ws is identity function \n",
    "otherwise if the dimensions are not same then Ws will be in such a way that it will make the dimensions of input and output equal.Ws is useful to change the dimensions.\n",
    "\n",
    "\n",
    "when we are using the L2 regularization(weight decay) and also on bias there is a possibility of shrinking the weight bias values to a very low value or even zero   which makes the initial layers of the network to learn the least features that is why the networks become slow and performance degrades at the same time the layers become ineffective then at that point of time a[l+2] will be equal to a[l]  \n",
    "\n",
    "\n",
    "when you see the above figure you will get some clarity.\n",
    "The skip connections are given in two ways\n",
    "- with normal lines\n",
    "- dotted lines.\n",
    "\n",
    "\n",
    "dotten lines means that Ws in skip connections is not an Identity function because the input and output dimensions are not equal.\n",
    "skips connections can be large or small but the output will be same.\n",
    "\n",
    "\n",
    "when you see the 34 layer network and resnet , resnet accuracy will be more because of skip connections.\n",
    "The addition of skipconnections have reduced the distance between the output and the input layers.\n",
    "when the input is given to first layer then it goes on till the last layer and loss is calculated.\n",
    "then the back propogation takes place from output to intput in between them a lot of calculation steps  will takes place based on the depth of the network.but when you are using the skip connections the no of steps to reach the initial layers of the network will be reduced and this prevents the weights in the initial layers from shriking to a very low value which is eventually protecting the network suffering with vanishing gradient descent.\n",
    "it also prevents the network from overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![residual block](residual_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=RYth6EbBUqM&t=205s\n",
    "https://www.youtube.com/watch?v=PQEKZDYyxFc\n",
    "https://pythonmachinelearning.pro/understanding-advanced-convolutional-neural-networks/\n",
    "https://medium.com/@pierre_guillou/understand-how-works-resnet-without-talking-about-residual-64698f157e0c\n",
    "https://cv-tricks.com/keras/understand-implement-resnets/#:~:text=ResNet%20uses%20Batch%20Normalization%20at,network%20from%20vanishing%20gradient%20problem.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below figure shows how the dimensions of the network decreases with each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![resnet layers](resnet_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
